# -*- coding: utf-8 -*-
import os

from datetime import datetime
from bazhouqu.kaipumodify.cfg import conf
import pytz
import requests
import xlrd
from openpyxl import load_workbook
from xlutils.copy import copy
from docx import Document
import pypandoc
import subprocess
import fitz
import sys
from pathlib import Path


def upload_new_file(jid, bz_gov_id, filename, columnId):
    cookies = {
        'authenticatecenterjsessionid': jid,
        'bz_govc_SHIROJSESSIONID': bz_gov_id,
    }

    headers = {
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'Connection': 'keep-alive',
        # 'Content-Type': 'multipart/form-data; boundary=----WebKitFormBoundaryi3TLT1PsLfm0Pa9i',
        'Origin': f'http://10.15.3.133:{conf.jiyuehua_port}',
        'Referer': f'http://10.15.3.133:{conf.jiyuehua_port}/ewebeditor/ewebeditor.htm?id=content&instanceid=content&style=Lstandard3&cusdir=/{conf.jiyuehua_siteid}/{columnId}&savepathfilename=d_savepathfilename&titleimage=1&extcss=/pageStyle/getCssByColumn/{columnId}&fixwidth=1078px',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0'
    }

    url = f'http://10.15.3.133:{conf.jiyuehua_port}/ewebeditor/jsp/upload.jsp?style=Lstandard3&cusdir=/{conf.jiyuehua_siteid}/{columnId}&skey=&h=10.15.3.133&o=http://10.15.3.133:{conf.jiyuehua_port}&action=mfu&type=file&blockflag=end'
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, 'downloadfile', filename)
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"å¾…ä¸Šä¼ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    try:
        with open(file_path, 'rb') as f:
            files = {
                'uploadfile': (filename, f, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'),
                'originalfilename': (None, filename)
            }
            response = requests.post(url, cookies=cookies, headers=headers, files=files, verify=False)
    except IOError as e:
        raise RuntimeError(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")

    print(f"å“åº”çŠ¶æ€ç : {response.status_code}")

    print(response.text)
    return response.json()['url']


def download_file(url, local_filename):
    response = requests.get(url, stream=True, verify=False)
    response.raise_for_status()

    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, 'downloadfile',local_filename)

    with open(file_path, 'wb') as file:
        for chunk in response.iter_content(chunk_size=8192):
            file.write(chunk)

    print(f"æ–‡ä»¶å·²ä¸‹è½½å¹¶ä¿å­˜ä¸º: {file_path}")
    # time.sleep(5)
    return file_path



def modify_file_xls(file_path, unique_replacements_list):
    # æ‰“å¼€xlsæ–‡ä»¶
    workbook_xls = xlrd.open_workbook(file_path, formatting_info=True)
    workbook_copy = copy(workbook_xls)


    # éå†æ‰€æœ‰çš„sheet
    for sheet_index in range(workbook_xls.nsheets):
        sheet_xls = workbook_xls.sheet_by_index(sheet_index)
        sheet_copy = workbook_copy.get_sheet(sheet_index)

        # éå†æ‰€æœ‰çš„å•å…ƒæ ¼
        for row in range(sheet_xls.nrows):
            for col in range(sheet_xls.ncols):
                cell_value = sheet_xls.cell_value(row, col)

                for i in unique_replacements_list:

                    if isinstance(cell_value, str) and i["sensitiveWords"] in cell_value:
                        new_value = cell_value.replace(i["sensitiveWords"], i["recommendUpdate"])
                        sheet_copy.write(row, col, new_value)
                        print(
                            f"Sheet '{sheet_xls.name}' ä¸­çš„å•å…ƒæ ¼ '{row + 1},{col + 1}' çš„å€¼å·²ä» '{cell_value}' æ›¿æ¢ä¸º '{new_value}'")

    # ä¿å­˜ä¿®æ”¹åçš„xlsæ–‡ä»¶
    workbook_copy.save(file_path)
    print(f"æ–‡ä»¶ '{file_path}' å·²æˆåŠŸä¿®æ”¹å¹¶ä¿å­˜")


def modify_file_xlsx(file_path, unique_replacements_list):
    # åŠ è½½å·¥ä½œç°¿
    workbook = load_workbook(file_path)

    # éå†æ‰€æœ‰çš„ sheet
    for sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]

        # éå†æ‰€æœ‰çš„å•å…ƒæ ¼
        for row in sheet.iter_rows():
            for cell in row:
                for i in unique_replacements_list:
                    if isinstance(cell.value, str) and i["sensitiveWords"] in cell.value:
                        new_value = cell.value.replace(i["sensitiveWords"], i["recommendUpdate"])
                        cell.value = new_value
                        print(f"Sheet '{sheet_name}' ä¸­çš„å•å…ƒæ ¼ '{cell.coordinate}' çš„å€¼å·²ä» '{i['sensitiveWords']}' æ›¿æ¢ä¸º '{i['recommendUpdate']}'")

    # ä¿å­˜ä¿®æ”¹åçš„å·¥ä½œç°¿
    workbook.save(file_path)
    print(f"æ–‡ä»¶ '{file_path}' å·²æˆåŠŸä¿®æ”¹å¹¶ä¿å­˜")


def modify_file_docx(file_path, unique_replacements_list):
    # åŠ è½½æ–‡æ¡£
    doc = Document(file_path)

    # éå†æ‰€æœ‰æ®µè½
    for para in doc.paragraphs:
        for i in unique_replacements_list:
            if i["sensitiveWords"] in para.text:
                new_text = para.text.replace(i["sensitiveWords"], i["recommendUpdate"])
                para.text = new_text
                print(f"æ®µè½ä¸­çš„æ–‡æœ¬å·²ä» '{i['sensitiveWords']}' æ›¿æ¢ä¸º '{i['recommendUpdate']}'")

    # éå†æ‰€æœ‰è¡¨æ ¼
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for i in unique_replacements_list:
                    if i["sensitiveWords"] in cell.text:
                        new_text = cell.text.replace(i["sensitiveWords"], i["recommendUpdate"])
                        cell.text = new_text
                        print(f"è¡¨æ ¼ä¸­çš„å•å…ƒæ ¼æ–‡æœ¬å·²ä» '{i['sensitiveWords']}' æ›¿æ¢ä¸º '{i['recommendUpdate']}'")

    # ä¿å­˜ä¿®æ”¹åçš„æ–‡æ¡£
    doc.save(file_path)
    print(f"æ–‡ä»¶ '{file_path}' å·²æˆåŠŸä¿®æ”¹å¹¶ä¿å­˜")




def modify_file(filename, item):
    script_dir = os.path.dirname(os.path.abspath(__file__))

    file_path = os.path.join(script_dir, 'downloadfile',filename)

    _, file_extension = os.path.splitext(file_path)

    unique_replacements = {}
    for i in item:
        key = (i['sensitiveWords'], i['recommendUpdate'])
        if key not in unique_replacements:
            unique_replacements[key] = i

    # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨
    unique_replacements_list = list(unique_replacements.values())

    if file_extension == '.xls':
        modify_file_xls(file_path, unique_replacements_list)
    elif file_extension == '.xlsx':
        modify_file_xlsx(file_path, unique_replacements_list)
    elif  file_extension == '.docx':
        modify_file_docx(file_path, unique_replacements_list)
    elif file_extension == '.doc' :
        modify_file_doc(file_path, unique_replacements_list)
        return 1
    elif file_extension == '.pdf' or file_extension == 'PDF':
        modify_file_pdf(file_path, unique_replacements_list)
    return 0


def generate_splits(old_text):
    """ç”Ÿæˆæ‰€æœ‰ä¸¤æ®µå¼æ‹†åˆ†ç»„åˆï¼ˆåŒ…å«å®Œæ•´æœªæ‹†åˆ†æƒ…å†µï¼‰"""
    splits = [(old_text[:i], old_text[i:]) for i in range(len(old_text), 0, -1)]
    splits.append((old_text, ""))  # åŒ…å«å®Œæ•´æ–‡æœ¬æƒ…å†µ
    return splits


def find_crossline_matches(page, old_text):
    text_blocks = [b for b in page.get_text("blocks") if b[6] == 0]
    matches = []

    # éå†æ‰€æœ‰å¯èƒ½çš„æ‹†åˆ†ç»„åˆï¼ˆä¼˜å…ˆæœ€é•¿å‰ç¼€ï¼‰
    for prefix, suffix in generate_splits(old_text):
        for i in range(len(text_blocks) - 1):
            curr_block = text_blocks[i][4].replace('\n', '')
            next_block = text_blocks[i + 1][4].replace('\n', '')

            # å½“å‰å—å¿…é¡»ä¸¥æ ¼ä»¥prefixç»“å°¾
            if not curr_block.endswith(prefix):
                continue

            # ä¸‹ä¸€ä¸ªå—å¿…é¡»ä¸¥æ ¼ä»¥suffixå¼€å¤´
            if not next_block.startswith(suffix):
                continue

            # è®¡ç®—åˆå¹¶åŒºåŸŸ
            rect_curr = fitz.Rect(text_blocks[i][:4])
            rect_next = fitz.Rect(text_blocks[i + 1][:4])
            combined_rect = rect_curr | rect_next

            matches.append({
                "rects": [rect_curr, rect_next],  # ä¿ç•™åŸå§‹åæ ‡ä¿¡æ¯
                "combined": combined_rect,  # åˆå¹¶åçš„åŒºåŸŸåæ ‡
                "original_split": (prefix, suffix),  # åŸå§‹æ–‡æœ¬æ‹†åˆ†æ–¹å¼
                # "prev_block_text": text_blocks[i][4],  # å‰ä¸€ä¸ªå—çš„åŸå§‹æ–‡æœ¬ï¼ˆå«æ¢è¡Œç¬¦ï¼‰
                # "next_block_text": text_blocks[i + 1][4],  # åä¸€ä¸ªå—çš„åŸå§‹æ–‡æœ¬
                "clean_prev": curr_block,  # å‰ä¸€ä¸ªå—å¤„ç†åçš„æ–‡æœ¬ï¼ˆå·²å»æ¢è¡Œï¼‰
                "clean_next": next_block  # åä¸€ä¸ªå—å¤„ç†åçš„æ–‡æœ¬
            })
            break  # ä¼˜å…ˆå¤„ç†æœ€é•¿åŒ¹é…

    return matches


def smart_replace(page, old_text, new_text):

    # å•è¡Œæ›¿æ¢
    single_matches = page.search_for(old_text)

    for rect in single_matches:
        print(f"  ğŸ–ï¸ æ­£åœ¨æ›¿æ¢å•è¡Œæ–‡æœ¬ï¼ˆï¼‰")
        print(f"   â†’ ä½ç½®ï¼š{rect} | åŸæ–‡æœ¬ï¼š'{old_text}' â†’ æ–°æ–‡æœ¬ï¼š'{new_text}'")
        print(f"   â†’ å­—ä½“ï¼šchina-s 11pt é¢œè‰²ï¼šé»‘è‰²")
        page.add_redact_annot(rect)
        page.apply_redactions()
        page.insert_text(rect.bl + (0, -5), new_text,
                         fontsize=11,
                         fontname="china-s",
                         color=(0, 0, 0))

        # è·¨è¡Œæ›¿æ¢
    cross_matches = find_crossline_matches(page, old_text)

    for match in cross_matches:
        print(f"\n  ğŸŒ å¤„ç†è·¨è¡ŒåŒ¹é…")
        print(f"   â†’ æ‹†åˆ†æ–¹å¼ï¼š'{match['original_split'][0]}' + '{match['original_split'][1]}'")
        print(f"   â†’ åˆå¹¶åŒºåŸŸï¼š{match['combined']}")

        # æ˜¾ç¤ºå®é™…ä¿®æ”¹å†…å®¹
        final_text = f"{match['clean_prev'][:-len(match['original_split'][0])]}{new_text}{match['clean_next'][len(match['original_split'][1]):]}"
        print(f"   â†’ ç”Ÿæˆæ–‡æœ¬ï¼š'{final_text[:30]}...'")

        # åˆ é™¤åŸåŒºåŸŸæç¤º
        print(f"   ğŸ—‘ï¸ åˆ é™¤åŸå§‹åŒºåŸŸï¼š")
        # åˆ é™¤åŸå§‹åŒºåŸŸ
        for rect in match["rects"]:
            page.add_redact_annot(rect)
        page.apply_redactions()

        # æ„å»ºå®Œæ•´æ–‡æœ¬å†…å®¹
        prefix, suffix = match["original_split"]
        prev_remain = match["clean_prev"][:-len(prefix)] if match["clean_prev"].endswith(prefix) else match[
            "clean_prev"]
        next_remain = match["clean_next"][len(suffix):] if match["clean_next"].startswith(suffix) else match[
            "clean_next"]

        # æ„å»ºæœ€ç»ˆå®Œæ•´æ–‡æœ¬ï¼ˆå‰å†…å®¹+æ–°å†…å®¹+åå†…å®¹ï¼‰
        final_text = f"{prev_remain}{new_text}{next_remain}"

        # å°†å®Œæ•´æ–‡æœ¬å¹³å‡åˆ†å‰²ä¸ºä¸¤è¡Œï¼ˆç²¾ç¡®å±…ä¸­åˆ†å‰²ï¼‰
        split_index = len(final_text) // 2  # æ•´ä½“åˆ†å‰²ç‚¹
        line1_text = final_text[:split_index]
        line2_text = final_text[split_index:]

        # è®¡ç®—æ’å…¥ä½ç½®ï¼ˆåŸºäºåˆå¹¶åŒºåŸŸçš„å‚ç›´å±…ä¸­åŸºå‡†ç‚¹ï¼‰
        combined_rect = match["combined"]
        font_size = 10
        line_spacing = font_size * 1.5  # è¡Œé—´è·

        # åŸºå‡†ç‚¹è®¡ç®—ï¼ˆåœ¨åˆå¹¶åŒºåŸŸå‚ç›´å±…ä¸­ä½ç½®ï¼‰
        base_y = combined_rect.y0 + (combined_rect.height - line_spacing) / 2  # ä¸¤è¡Œæ•´ä½“å‚ç›´å±…ä¸­
        base_point = (combined_rect.x0, base_y)  # å·¦å¯¹é½åŸºå‡†ç‚¹

        # ä¸¤è¡Œä½ç½®è®¡ç®—ï¼ˆPyMuPDFåæ ‡ç³»yè½´å‘ä¸‹å¢é•¿ï¼‰
        line1_pos = base_point
        line2_pos = (base_point[0], base_point[1] + line_spacing)
        print(f"   âœï¸ æ’å…¥æ–°æ–‡æœ¬ï¼ˆå­—ä½“ï¼šchina-s 10pt é¢œè‰²ï¼šé»‘è‰²ï¼‰")
        print(f"    è¡Œ1ä½ç½®ï¼š{match['combined'].x0, match['combined'].y0 + 10}")
        print(f"    è¡Œ2ä½ç½®ï¼š{match['combined'].x0, match['combined'].y0 + 25}")
        # æ’å…¥ä¸¤è¡Œæ–‡æœ¬ï¼ˆç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
        page.insert_text(
            line1_pos,
            line1_text,
            fontsize=font_size,
            color=(0, 0, 0),
            fontname="china-s",
            overlay=True
        )

        page.insert_text(
            line2_pos,
            line2_text,
            fontsize=font_size,
            color=(0, 0, 0),
            fontname="china-s",
            overlay=True
        )

def modify_file_pdf(file_path, unique_replacements_list):
    doc = fitz.open(file_path)
    for page in doc:
        for replacement in unique_replacements_list:
            sensitive_word = replacement["sensitiveWords"]
            new_word = replacement["recommendUpdate"]
            smart_replace(page,sensitive_word,new_word)
    temp_file = file_path.replace(".pdf", "_temp.pdf")
    doc.save(temp_file, garbage=4, deflate=True)
    doc.close()

    # ç”¨ä¸´æ—¶æ–‡ä»¶æ›¿æ¢åŸæ–‡ä»¶
    os.replace(temp_file, file_path)
    print("\nâœ… æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆ")
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¿®æ”¹åˆ°: {file_path}")

def modify_file_doc(file_path_old, unique_replacements_list):
    try:
        output = doc_to_docx(file_path_old)
        print(f"è½¬æ¢æˆåŠŸï¼æ–‡ä»¶ä¿å­˜è‡³ï¼š{output}")
        print(f"æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(output) / 1024:.2f} KB")
    except Exception as e:
        print(f"é”™è¯¯ï¼š{str(e)}")
    # file_path_new = file_path_old.replace('.doc', '.docx')
    # pypandoc.convert_file(file_path_old, 'docx', outputfile=file_path)
    modify_file_docx(output, unique_replacements_list)


def uploadfile(jid, bz_gov_id, file_name, path_excel, parent_t, article_t, content_id):
    cookies = {
        'historyCookie': '%E5%8E%BF%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E5%8A%9E%E5%85%AC%E5%AE%A4%2C%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E6%95%99%E8%82%B2%E9%83%A8%2C%E4%BA%BA%E6%B0%91%E6%97%A5%E6%8A%A5%2C%E5%8E%BF%E8%87%AA%E7%84%B6%E8%B5%84%E6%BA%90%E5%92%8C%E8%A7%84%E5%88%92%E5%B1%80%2C%E5%9B%9B%E5%B7%9D%E7%9C%81%E6%95%99%E8%82%B2%E8%80%83%E8%AF%95%E9%99%A2%2C%E5%9B%BD%E7%BD%91%E5%B9%B3%E6%98%8C%E5%8E%BF%E4%BE%9B%E7%94%B5%E5%85%AC%E5%8F%B8%2C%E5%B9%B3%E6%98%8C%E5%8E%BF%E4%BA%BA%E6%B0%91%E6%94%BF%E5%BA%9C%E5%8A%9E%E5%85%AC%E5%AE%A4%2C%E4%BA%BA%E6%B0%91%E7%BD%91%2C%E5%B9%B3%E6%98%8C%E5%8E%BF%E5%8F%B8%E6%B3%95%E5%B1%80%2C%E5%B9%B3%E6%98%8C%E5%8E%BF%E7%BB%BC%E5%90%88%E8%A1%8C%E6%94%BF%E6%89%A7%E6%B3%95%E5%B1%80',
        'authenticatecenterjsessionid': jid,
        'bz_govc_SHIROJSESSIONID': bz_gov_id,
    }

    headers = {
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'Connection': 'keep-alive',
        'Origin': 'http://10.15.3.133:'+conf.jiyuehua_port,
        'Referer': 'http://10.15.3.133:83/fileCenter/uploadPage?uploadType=1&s=0.1754363371550416&isOpen=true&_=1722305351088',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0',
    }

    # è·å–å½“å‰æ—¶é—´å¹¶è®¾ç½®ä¸ºä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆCSTï¼‰
    current_time = datetime.now(pytz.timezone('Asia/Shanghai'))
    formatted_time = current_time.strftime('%a %b %d %Y %H:%M:%S GMT%z (ä¸­å›½æ ‡å‡†æ—¶é—´)')

    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, 'downloadfile',file_name)

    files = {
        'siteId': (None, conf.jiyuehua_siteid),
        'sessionId': (None, bz_gov_id),
        'type': (None, 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'),
        'title': (None, parent_t),
        'contentId': (None, content_id),
        'filePath': (None, path_excel),
        'fileName': (None, article_t),
        'id': (None, 'WU_FILE_0'),
        'name': (None, file_name),
        'lastModifiedDate': (None, formatted_time),
        # 'size': (None, '43008'),
        'Filedata': (file_name, open(file_path, 'rb'), 'application/vnd.ms-excel'),
    }

    response = requests.post(
        'http://10.15.3.133:'+conf.jiyuehua_port+'/content/updateFileByPath',
        cookies=cookies,
        headers=headers,
        files=files,
        verify=False,
    )

    print(response.status_code)
    print(response.text)
    return response.json()




def get_libreoffice_path():
    """è‡ªåŠ¨è·å–å„å¹³å°çš„LibreOfficeè·¯å¾„"""
    if sys.platform == 'win32':
        # Windowsé»˜è®¤å®‰è£…è·¯å¾„ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…å®‰è£…ä½ç½®è°ƒæ•´ï¼‰
        paths = [
            r"E:\soft3\libreoffice\program\soffice.exe"
        ]
    else:  # Linux/macOS
        paths = ["/usr/bin/libreoffice"]

    for path in paths:
        if os.path.exists(path):
            return path
    raise FileNotFoundError("æœªæ‰¾åˆ°LibreOfficeï¼Œè¯·ç¡®è®¤å·²å®‰è£…å¹¶æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„")


def doc_to_docx(file_path):
    """è·¨å¹³å°docè½¬docxï¼ˆä½¿ç”¨LibreOfficeï¼‰"""
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    # å‡†å¤‡è¾“å‡ºè·¯å¾„
    output_dir = os.path.dirname(file_path)
    output_path = Path(file_path).with_suffix('.docx')

    # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
    command = [
        get_libreoffice_path(),
        "--headless",
        "--convert-to", "docx",
        "--outdir", output_dir,
        file_path
    ]

    try:
        # æ‰§è¡Œè½¬æ¢ï¼ˆè¶…æ—¶60ç§’ï¼‰
        result = subprocess.run(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            timeout=60
        )
    except subprocess.TimeoutExpired:
        raise RuntimeError("æ–‡æ¡£è½¬æ¢è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå")

    # æ£€æŸ¥è½¬æ¢ç»“æœ
    if result.returncode != 0:
        error_msg = result.stderr.decode('utf-8', errors='ignore')
        raise RuntimeError(f"è½¬æ¢å¤±è´¥: {error_msg}")

    if not output_path.exists():
        available_files = "\n".join(os.listdir(output_dir))
        raise FileNotFoundError(
            f"è¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆï¼Œç›®å½•ä¸­å­˜åœ¨ï¼š\n{available_files}"
        )

    return str(output_path)

def find_and_replace_text(input_file, output_file, old_text, new_text):
    doc = fitz.open(input_file)

    # æ­£ç¡®è·å–PDFç‰ˆæœ¬çš„æ–¹å¼
    pdf_version = doc.metadata.get("format", "æœªçŸ¥ç‰ˆæœ¬").split()[-1]
    print(f"PDFè§„èŒƒç‰ˆæœ¬ï¼šISO 32000-1:{pdf_version} (å¯¹åº”PDF {doc.metadata['format']})")

    for page in doc:
        # æ–‡æœ¬å­˜åœ¨æ€§éªŒè¯ï¼ˆå¼ºåŒ–ç‰ˆï¼‰
        text_blocks = page.get_text("blocks")
        for i, block in enumerate(text_blocks):
            print(f"å— {i} è¯¦ç»†ä¿¡æ¯:")
            print(f"åæ ‡èŒƒå›´ï¼š({block[0]}, {block[1]}) â†’ ({block[2]}, {block[3]})")
            print(f"å†…å®¹ï¼š{block[4]}")
            print(f"å—ç±»å‹ï¼š{'æ–‡æœ¬' if block[6] == 0 else 'éæ–‡æœ¬'}\n{'-' * 40}")
        found = any(old_text in block[4] for block in text_blocks if block[6] == 0)
        print(f"æ–‡æœ¬å­˜åœ¨æ€§äºŒæ¬¡éªŒè¯ï¼š{'æ‰¾åˆ°' if found else 'æœªæ‰¾åˆ°'}")

        text_instances = page.search_for(old_text)
        print(f"åŸå§‹åŒ¹é…ç»“æœï¼š{len(text_instances)}å¤„")

        for rect in text_instances:
            # æ·»åŠ è°ƒè¯•æ ‡è®°
            red_rect = page.add_redact_annot(rect, fill=(1, 1, 0.8))  # é»„è‰²èƒŒæ™¯
            page.apply_redactions()

            # å¼ºåˆ¶ä½¿ç”¨å†…ç½®å­—ä½“
            page.insert_text(
                rect.bl + (0, -rect.height * 0.1),  # åŸºçº¿æ ¡å‡†
                new_text,
                fontsize=rect.height * 0.8,
                fontname="china-s",  # é‡è¦ä¿®æ­£ï¼šæŒ‡å®šå†…ç½®ä¸­æ–‡å­—ä½“
                color=(1, 0, 0),
                overlay=True
            )
            print(f"å·²å†™å…¥ï¼š{new_text} | åæ ‡ï¼š{tuple(rect)}")

    # ç‰ˆæœ¬å…¼å®¹çš„ä¿å­˜æ–¹å¼
    doc.save(output_file, garbage=4, deflate=True, clean=True)
    print(f"æœ€ç»ˆæ–‡ä»¶å¤§å°ï¼š{os.path.getsize(output_file) // 1024}KB")


# ç¯å¢ƒæ£€æŸ¥ï¼ˆå¿…é¡»è¿è¡Œï¼‰
print(f"PyMuPDFç‰ˆæœ¬ï¼š{fitz.version}")  # æ­£ç¡®ç‰ˆæœ¬è·å–æ–¹å¼

import fitz
from itertools import product


def split_all_combinations(word, max_split=3):
    """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç»„åˆï¼Œæœ€å¤šæ‹†åˆ†ä¸º3éƒ¨åˆ†"""
    return [(word[:i], word[i:]) for i in range(1, len(word)) if i <= max_split or (len(word) - i) <= max_split]


def find_crossline_instances(page, old_text):
    text_blocks = [b for b in page.get_text("blocks") if b[6] == 0]
    matches = []

    # éå†æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç»„åˆ
    for prefix, suffix in split_all_combinations(old_text):
        for i in range(len(text_blocks) - 1):
            curr_block = text_blocks[i][4].replace('\n', '')
            next_block = text_blocks[i + 1][4].replace('\n', '')

            # æ£€æŸ¥è·¨å—åŒ¹é…
            if curr_block.endswith(prefix) and next_block.startswith(suffix):
                # è®¡ç®—åˆå¹¶åŒºåŸŸ
                rect1 = fitz.Rect(text_blocks[i][:4])
                rect2 = fitz.Rect(text_blocks[i + 1][:4])
                combined_rect = rect1 | rect2

                matches.append({
                    "prefix_rect": rect1,
                    "suffix_rect": rect2,
                    "combined_rect": combined_rect,
                    "split_point": len(prefix)
                })

    return matches


def find_and_replace_text2(input_file, output_file, old_text, new_text):
    doc = fitz.open(input_file)

    for page in doc:
        # é¦–å…ˆå¤„ç†å•è¡ŒåŒ¹é…
        single_line_matches = page.search_for(old_text)
        for rect in single_line_matches:
            page.add_redact_annot(rect)
            page.apply_redactions()
            page.insert_text(
                rect.bl + (0, -rect.height * 0.1),
                new_text,
                fontsize=rect.height * 0.8,
                fontname="china-s",
                color=(1, 0, 0),
                overlay=True
            )

        # å¤„ç†è·¨è¡ŒåŒ¹é…
        cross_matches = find_crossline_instances(page, old_text)
        for match in cross_matches:
            # åˆ›å»ºè·¨è¡ŒåŒºåŸŸçš„çº¢è‰²åˆ é™¤çº¿
            page.add_redact_annot(match["prefix_rect"])
            page.add_redact_annot(match["suffix_rect"])
            page.apply_redactions()

            # åœ¨åˆå¹¶åŒºåŸŸå±…ä¸­æ’å…¥æ–°æ–‡æœ¬
            center_point = match["combined_rect"].bl + (0, -match["combined_rect"].height * 0.2)
            page.insert_text(
                center_point,
                new_text,
                fontsize=match["combined_rect"].height * 0.7,
                fontname="china-s",
                color=(1, 0, 0),
                overlay=True
            )

    doc.save(output_file, garbage=4, deflate=True, clean=True)


if __name__ == "__main__":
    input_file = r"G:\project\python\pcxzf\pcxzf\bazhouqu\kaipumodify\modifyfile\downloadfile\20180904150156-752201\20180904150156-752201_101.pdf"
    output_file = r"G:\project\python\pcxzf\pcxzf\bazhouqu\kaipumodify\modifyfile\downloadfile\20180904150156-752201\modified_output.pdf"

    # ç¤ºä¾‹æ›¿æ¢å‚æ•°ï¼ˆéœ€è¦æ‚¨è‡ªè¡Œä¿®æ”¹ï¼‰
    old_text = "ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•æ”¹é©å§”å‘˜ä¼š"
    new_text = "ä¸­åäººæ°‘å…±å’Œå›½å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š"

    find_and_replace_text2(input_file, output_file, old_text, new_text)
